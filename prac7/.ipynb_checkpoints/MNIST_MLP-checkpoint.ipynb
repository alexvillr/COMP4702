{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "MPS Available : True\n",
      "CUDA Available : False\n",
      "DEVICE - mps\n",
      "Python 3.10.10\n"
     ]
    }
   ],
   "source": [
    "# print sanity check of versions and device\n",
    "DEVICE = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(torch.__version__)\n",
    "print(f\"MPS Available : {torch.backends.mps.is_built()}\")\n",
    "print(f\"CUDA Available : {torch.cuda.is_available()}\")\n",
    "print(f\"DEVICE - {DEVICE}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10_data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=tf.Compose(\n",
    "        [\n",
    "            tf.ToTensor(),\n",
    "            tf.Normalize(\n",
    "                mean=(0.49139968, 0.48215827, 0.44653124),\n",
    "                std=(0.24703233, 0.24348505, 0.26158768),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10_data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=tf.Compose(\n",
    "        [\n",
    "            tf.ToTensor(),\n",
    "            tf.Normalize(\n",
    "                mean=(0.49139968, 0.48215827, 0.44653124),\n",
    "                std=(0.24703233, 0.24348505, 0.26158768),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseOnlyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseOnlyCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Flatten(),\n",
    "            nn.Linear(1850, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(32, 7),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 130\n",
    "# wd = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = DenseOnlyCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterating over the training dataset in batches\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            y_true = labels.to(DEVICE)\n",
    "\n",
    "            # Calculating outputs for the batch being iterated\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculated prediction labels from models\n",
    "            _, y_pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Comparing predicted and true labels\n",
    "            test_acc += (y_pred == y_true).sum().item()\n",
    "\n",
    "        print(f\"Test set accuracy = {100 * test_acc / len(test_set)} %\")\n",
    "        return 100 * test_acc / len(test_set)\n",
    "\n",
    "\n",
    "def get_loss():\n",
    "    train_loss = 0\n",
    "\n",
    "    # Iterating over the training dataset in batches\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Extracting images and target labels for the batch being iterated\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Calculating the model output and the cross entropy loss\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Updating weights according to calculated loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Printing loss for each epoch\n",
    "    train_loss_list.append(train_loss / len(train_loader))\n",
    "    print(f\"Training loss = {train_loss_list[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "model = model.to(device=DEVICE)\n",
    "best_acc = 0\n",
    "best_epoch = -1\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} +++++++++++++++++\")\n",
    "    get_loss()\n",
    "    this_acc = get_accuracy()\n",
    "    if this_acc > best_acc:\n",
    "        best_acc = this_acc\n",
    "        best_epoch = epoch\n",
    "print(f\"Best accuracy occurred at {best_epoch} and was: {best_acc} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
